{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packman Deep Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackmanDQN:\n",
    "    def __init__(self, session, inputShape, outputShape, name):\n",
    "        self._sess = session\n",
    "        self._inputShape = list(inputShape)\n",
    "        self._outputShape = outputShape\n",
    "        self._name = name\n",
    "        self.BuildNetwork()\n",
    "    \n",
    "    def BuildNetwork(self):\n",
    "        with tf.variable_scope(self._name, reuse=tf.AUTO_REUSE):\n",
    "            self._X = tf.placeholder(tf.float32, shape=[None, 210,160,3])\n",
    "            \n",
    "            #X shape = [None, 210, 160, 3]\n",
    "            W1 = tf.get_variable(name=\"_Weight1\", shape=[3, 3, 3, 64], initializer=tf.glorot_normal_initializer())\n",
    "            L1 = tf.nn.conv2d(self._X, W1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "            #shape reduces to [105, 80, 96]\n",
    "            \n",
    "            W2 = tf.get_variable(name=\"_Weight2\", shape=[3, 3, 64, 128], initializer=tf.glorot_normal_initializer())\n",
    "            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "            L2 = tf.nn.relu(L2)\n",
    "            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "            #L2 = tf.layers.batch_normalization(L2)\n",
    "            #shape reduces to [53, 40, 192]\n",
    "            \n",
    "            #W3 = tf.get_variable(name=\"filter3\", shape=[3, 3, 192, 384], initializer=tf.glorot_normal_initializer())\n",
    "            #L3 = tf.nn.conv2d(L2, W3, strides=[1,1,1,1], padding=\"SAME\")\n",
    "            #L3 = tf.nn.relu(L3)\n",
    "            #L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "            #L3 = tf.nn.dropout(L3, keep_prob=self._keep_prob)\n",
    "            #L3 = tf.layers.batch_normalization(L3)\n",
    "            #shape reduces to [27, 20, 256]\n",
    "            \n",
    "            W4 = tf.get_variable(name=\"_Weight4\", shape=[3, 3, 128, 256], initializer=tf.glorot_normal_initializer())\n",
    "            L4 = tf.nn.conv2d(L2, W4, strides=[1,1,1,1], padding=\"SAME\")\n",
    "            L4 = tf.nn.relu(L4)\n",
    "            L4 = tf.nn.max_pool(L4, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "            #L4 = tf.layers.batch_normalization(L4)\n",
    "            #shape reduces to [14, 10, 384]\n",
    "            \n",
    "            W5 = tf.get_variable(name=\"_Weight5\", shape=[3, 3, 256, 256], initializer=tf.glorot_normal_initializer())\n",
    "            L5 = tf.nn.conv2d(L4, W5, strides=[1,1,1,1], padding=\"SAME\")\n",
    "            L5 = tf.nn.relu(L5)\n",
    "            L5 = tf.nn.max_pool(L5, ksize=[1, 2, 2, 1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "            #L5 = tf.layers.batch_normalization(L5)\n",
    "            #shape reduces to [7, 5, 384]\n",
    "            L5 = tf.reshape(L5, [-1, 7*5*256])\n",
    "            \n",
    "            W6 = tf.get_variable(name=\"fc1\", shape=[7*5*256, 256], initializer=tf.glorot_normal_initializer())\n",
    "            b6 = tf.Variable(tf.random_normal(shape=[256], stddev=0.005), name=\"b6\")\n",
    "            H6 = tf.nn.relu(tf.matmul(L5, W6) + b6)\n",
    "            \n",
    "            #W7 = tf.get_variable(name=\"fc2\", shape=[1280, 128], initializer=tf.glorot_normal_initializer())\n",
    "            #b7 = tf.Variable(tf.random_normal(shape=[128], stddev=0.005), name=\"b7\")\n",
    "            #H7 = tf.nn.relu(tf.matmul(H6, W7) + b7)\n",
    "            \n",
    "            W8 = tf.get_variable(name=\"fc3\", shape=[256, outputShape], initializer=tf.glorot_normal_initializer())\n",
    "            b8 = tf.Variable(tf.random_normal(shape=[outputShape], stddev=0.005), name=\"b8\")\n",
    "            self._hypothesis = tf.matmul(H6, W8) + b8\n",
    "            \n",
    "            self._Y = tf.placeholder(tf.float32, shape=[None, outputShape])\n",
    "            self._loss = tf.reduce_mean(tf.square(self._hypothesis - self._Y))\n",
    "            self._train = tf.train.AdamOptimizer(0.001).minimize(self._loss)\n",
    "        \n",
    "    def predict(self, state, keep_prob):\n",
    "        x = np.reshape(state, [1, 210, 160, 3])\n",
    "        return self._sess.run(self._hypothesis, feed_dict={self._X: x})\n",
    "    \n",
    "    def update(self, x_stack, y_stack, keep_prob):\n",
    "        return self._sess.run([self._loss, self._train], feed_dict={self._X: x_stack, self._Y: y_stack})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('MsPacman-v0')\n",
    "inputShape = env.observation_space.shape\n",
    "outputShape = env.action_space.n\n",
    "\n",
    "maxEpisode= 100\n",
    "dis = 0.9\n",
    "replayList = deque()\n",
    "replayMaxSize = 20000\n",
    "\n",
    "def GetCopyVarOperation(predNetworkName=\"pred\", targetNetworkName=\"target\"):\n",
    "    copyOperation =[]\n",
    "    predVars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"pred\")\n",
    "    targetVars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"target\")\n",
    "    \n",
    "    for predvar, targetvar in zip(predVars, targetVars):\n",
    "        copyOperation.append(targetvar.assign(predvar))\n",
    "    return copyOperation\n",
    "\n",
    "def Replay(predNet, targetNet, batch):\n",
    "    x_stack = np.empty(0).reshape(0, 210, 160, 3)\n",
    "    y_stack = np.empty(0).reshape(0, outputShape)\n",
    "    \n",
    "    for state, action, reward, done, nextState in batch:\n",
    "        #Q is [[4]] shaped array. Q[0, action] ==> nextState\n",
    "        Q = predNet.predict(state)\n",
    "\n",
    "        if done: Q[0, action] = reward\n",
    "        else: Q[0, action] = reward + dis * np.max(targetNet.predict(nextState))\n",
    "\n",
    "        x_stack = np.vstack([x_stack, [state]])\n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "\n",
    "    return predNet.update(x_stack, y_stack)\n",
    "\n",
    "def BotPlay(predNet):\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(predNet.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(\"Total Score: {}\".format(reward_sum))\n",
    "            break\n",
    "    env.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "INFO:tensorflow:Restoring parameters from ./train_model.ckpt\n"
    },
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'keep_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-96db59e887c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mBotPlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredNet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-b0d3e4780205>\u001b[0m in \u001b[0;36mBotPlay\u001b[1;34m(predNet)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'keep_prob'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    predNet = PackmanDQN(sess, inputShape, outputShape, \"pred\")\n",
    "    targetNet = PackmanDQN(sess, inputShape, outputShape, \"target\")\n",
    "\n",
    "    save_file = './train_model.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, save_file)\n",
    "    BotPlay(predNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tf.Session() as sess:\n",
    "    predNet = PackmanDQN(sess, inputShape, outputShape, \"pred\")\n",
    "    targetNet = PackmanDQN(sess, in putShape, outputShape, \"target\")\n",
    "    copyOperation = GetCopyVarOperation(\"pred\", \"target\")\n",
    "    random.seed(time.time())\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    save_file = './train_model.ckpt'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, save_file)\n",
    "    reward_avg=0\n",
    "    for episode in range(1, maxEpisode):\n",
    "        \n",
    "        e = 1. / ((episode / 10) + 1)\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewardSum = 0\n",
    "        \n",
    "        while not done:\n",
    "            if random.random() < e:\n",
    "                action = random.randint(0, 8)\n",
    "            else:\n",
    "                action = np.argmax(predNet.predict(state, 0.5))\n",
    "                \n",
    "            nextState, reward, done, info = env.step(action)\n",
    "            if done: reward = -100\n",
    "                \n",
    "            replayList.append((state, action, reward, done, nextState))\n",
    "            if len(replayList) > replayMaxSize:\n",
    "                replayList.popleft()\n",
    "            \n",
    "            rewardSum += reward\n",
    "            reward_avg += reward/10\n",
    "            state = nextState\n",
    "            if rewardSum>5000: break\n",
    "        \n",
    "        print('Episode: {}, Reward: {}'.format(episode, rewardSum))\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            loss_avg=0\n",
    "            batch = random.sample(replayList, 10)\n",
    "            for i in range(50):\n",
    "                loss, _ = Replay(predNet, targetNet, batch)\n",
    "                loss_avg += loss / 50\n",
    "            sess.run(copyOperation)\n",
    "            print('Loss: {}, Average Reward: {}'.format(loss_avg, reward_avg))\n",
    "            reward_avg=0\n",
    "    \n",
    "    saver.save(sess, save_file)\n",
    "    BotPlay(predNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}